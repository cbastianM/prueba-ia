# -------------------
# LIBRER√çAS (Versi√≥n Simplificada)
# -------------------
import streamlit as st
import google.generativeai as genai
import pandas as pd
import numpy as np
import re

# -------------------
# CONFIGURACI√ìN DE LA P√ÅGINA Y API
# -------------------
st.set_page_config(page_title="Profesor de Ing. Civil", page_icon="üéì")
st.title("üéì Profesor Virtual de Ingenier√≠a Civil")

try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=GEMINI_API_KEY)
except KeyError:
    st.error("‚ö†Ô∏è No se encontr√≥ la GEMINI_API_KEY.")
    st.stop()

# -------------------
# CARGA DE DATOS DESDE CSV EN GITHUB
# -------------------
@st.cache_data(ttl=3600)
def load_knowledge_base():
    """Lee el CSV desde una URL fija de GitHub."""
    csv_url = "https://raw.githubusercontent.com/cbastianM/prueba-ia/main/Conocimiento_Ing_Civil.csv"
    try:
        df = pd.read_csv(csv_url, skipinitialspace=True)
        df.fillna('', inplace=True)
        return df
    except Exception as e:
        st.error(f"‚ùå Error al leer el archivo CSV desde GitHub: {e}")
        return None

# -------------------
# L√ìGICA DEL CHATBOT (SIMPLIFICADA)
# -------------------
def generate_response(query, dataframe):
    """
    Genera una respuesta de texto y una cadena Markdown para mostrar la imagen.
    """
    # Limpieza y b√∫squeda de la fila
    query_cleaned = query.strip().lower()
    match_df = dataframe[dataframe['ID_Ejercicio'].str.strip().str.lower() == query_cleaned]
    
    if match_df.empty:
        return f"Lo siento, no pude encontrar el ejercicio '{query}'. Por favor, aseg√∫rate de escribir el ID exacto (ej: 'ejercicio 1.1').", ""

    match_row = match_df.iloc[0]

    # --- L√ìGICA DE IMAGEN CON MARKDOWN ---
    context_text = match_row['Contenido']
    image_url = match_row['URL_Imagen']
    image_markdown = "" # String vac√≠o por defecto
    
    if image_url and isinstance(image_url, str):
        # Limpiamos la URL por si acaso
        cleaned_url = image_url.strip().strip("'\"")
        # Creamos la cadena de Markdown para la imagen
        image_markdown = f"![Diagrama del Ejercicio]({cleaned_url})"
            
    # --- GENERACI√ìN DE LA RESPUESTA DE LA IA (SOLO TEXTO) ---
    model = genai.GenerativeModel('gemini-1.5-flash-latest') # Usamos un modelo m√°s r√°pido y econ√≥mico
    
    prompt = f"""
    **ROL:** Eres un profesor de Ingenier√≠a Civil.
    **TAREA:** Explica la soluci√≥n al ejercicio bas√°ndote en la siguiente informaci√≥n. Tu explicaci√≥n debe ser clara y usar formato Markdown y LaTeX.
    
    **PREGUNTA DEL USUARIO:** {query}
    **INFORMACI√ìN DE LA SOLUCI√ìN:** {context_text}
    
    **TU EXPLICACI√ìN:**
    """
    
    try:
        response = model.generate_content(prompt)
        # Devolvemos el texto de la IA y la cadena de Markdown para la imagen
        return response.text, image_markdown
    except Exception as e:
        return f"Error al contactar a la IA: {e}", image_markdown


# -------------------
# INTERFAZ DE USUARIO (SIMPLIFICADA)
# -------------------
df_knowledge = load_knowledge_base()

if df_knowledge is not None:
    st.success("Base de conocimiento cargada. ¬°El profesor est√° listo!")
    if 'messages' not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "¬øQu√© ejercicio quieres revisar?"}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"]) # st.markdown mostrar√° tanto el texto como la imagen

    if prompt := st.chat_input("Escribe el ID del ejercicio (ej: ejercicio 1.1)"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Preparando la explicaci√≥n..."):
                response_text, image_markdown = generate_response(prompt, df_knowledge)
            
            # Combinamos el markdown de la imagen con el texto de la respuesta
            full_response = f"{image_markdown}\n\n{response_text}"
            
            st.markdown(full_response)
            
            # Guardamos la respuesta completa en el historial
            assistant_message = {"role": "assistant", "content": full_response}
            st.session_state.messages.append(assistant_message)
else:
    st.error("La aplicaci√≥n no puede iniciar porque no se pudo cargar la base de conocimiento.")
