# -------------------
# LIBRER√çAS
# -------------------
import streamlit as st
import google.generativeai as genai
import pandas as pd
import numpy as np
import re

# -------------------
# CONFIGURACI√ìN DE LA P√ÅGINA Y API
# -------------------
st.set_page_config(
    page_title="Tu Profesor de Ing. Civil",
    page_icon="üéì",
    layout="centered" # Un layout m√°s enfocado para chat
)

st.title("üéì Tu Profesor Virtual de Ingenier√≠a Civil")
st.markdown("Hazme una pregunta sobre un tema o pide la explicaci√≥n de un ejercicio de la base de conocimiento.")

# Cargar la API Key desde los secrets de Streamlit
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=GEMINI_API_KEY)
except (FileNotFoundError, KeyError):
    st.error("‚ö†Ô∏è No se encontr√≥ la GEMINI_API_KEY. Por favor, config√∫rala en los secrets de Streamlit Cloud.")
    st.stop()

# -------------------
# BASE DE CONOCIMIENTO (DATOS DENTRO DEL C√ìDIGO)
# -------------------

# Reemplaza tu antigua funci√≥n get_knowledge_base() por esta:

@st.cache_resource
def get_knowledge_base():
    """
    Lee un archivo CSV desde una URL p√∫blica (GitHub), lo convierte a DataFrame
    y genera los embeddings. Retorna el DataFrame procesado.
    """
    # --- CONSTRUYE LA URL DEL ARCHIVO CSV EN GITHUB ---
    # Reemplaza 'tu_usuario_github', 'tu_repositorio' y 'main' si es necesario.
    github_user = "cbastianM"
    github_repo = "prueba-ia"
    branch_name = "main" # O 'master', dependiendo de tu repositorio
    file_path = "Conocimiento_Ing_Civil.csv"

     # La URL para acceder al archivo en formato "raw" (crudo)
    csv_url = f"https://raw.githubusercontent.com/{github_user}/{github_repo}/{branch_name}/{file_path}"
    
    st.info(f"Cargando base de conocimiento desde: {csv_url}")


    try:
        # Lee el archivo CSV directamente desde la URL
        df = pd.read_csv(csv_url)
        
        # Limpieza de datos: reemplaza valores nulos (NaN) por strings vac√≠os
        df.fillna('', inplace=True)
        
        # Generaci√≥n de embeddings
        model_embedding = 'models/embedding-001'
        
        # Usamos una barra de progreso para dar feedback al usuario durante la carga inicial
        progress_bar = st.progress(0, text="Analizando base de conocimiento...")
        
        def get_embedding(text, index):
            # Actualiza la barra de progreso
            progress_bar.progress((index + 1) / len(df), text=f"Procesando entrada {index+1}/{len(df)}...")
            if not isinstance(text, str) or not text.strip():
                return [0.0] * 768
            return genai.embed_content(
                model=model_embedding, content=text, task_type="RETRIEVAL_DOCUMENT")["embedding"]

        # Aplica la funci√≥n de embedding
        df['Embedding'] = [get_embedding(row['Contenido'], i) for i, row in df.iterrows()]
        
        # Limpia la barra de progreso una vez terminado
        progress_bar.empty()
        
        return df

    except Exception as e:
        st.error(f"Error al cargar o procesar el archivo CSV desde GitHub: {e}")
        st.warning("Verifica que la URL sea correcta y que el archivo CSV est√© en el repositorio.")
        return None

# -------------------
# L√ìGICA DEL CHATBOT (CEREBRO CON PERSONA DE PROFESOR)
# -------------------

# --- FUNCI√ìN AUXILIAR (ASEG√öRATE DE QUE EST√â AQU√ç) ---
def extract_exercise_id(query):
    """Extrae un ID de ejercicio de la pregunta del usuario."""
    # Puedes a√±adir m√°s nombres de libros a esta lista
    books = ["beer", "hibbeler", "singer", "gere", "chopra", "irving"]
    
    # Patr√≥n de expresi√≥n regular para encontrar "libro numero.numero"
    pattern = re.compile(
        r'\b(' + '|'.join(books) + r')'  # Busca una de las palabras de la lista de libros
        r'[\s\w\.]*'                      # Permite texto intermedio
        r'(\d+[\.\-]\d+)\b',              # Captura el n√∫mero del ejercicio como "2.73" o "4-5"
        re.IGNORECASE                     # Ignora may√∫sculas/min√∫sculas
    )
    
    match = pattern.search(query)
    if match:
        book_name = match.group(1).upper()
        exercise_num = match.group(2).replace('-', '.')
        return f"{book_name} {exercise_num}"
    return None

def generate_response(query, dataframe):
    """
    Genera una respuesta con un modelo de IA con dos personalidades:
    1. ESTRICTO para ejercicios: Solo usa la base de datos.
    2. FLEXIBLE para teor√≠a: Usa su conocimiento general, enriquecido opcionalmente por la base de datos.
    """
    model_generation = genai.GenerativeModel('gemini-1.5-flash-latest')
    
    # --- PASO 1: Determinar si la pregunta es sobre un ejercicio espec√≠fico ---
    exercise_id = extract_exercise_id(query)
    
    # --- CAMINO A: PREGUNTA SOBRE UN EJERCICIO (PERSONALIDAD ESTRICTA) ---
    if exercise_id:
        print(f"DEBUG: ID de ejercicio detectado: '{exercise_id}'. Entrando en CAMINO ESTRICTO.") # Mensaje de depuraci√≥n
        
        match = dataframe[dataframe['ID_Ejercicio'].str.strip().str.upper() == exercise_id.upper()]
        
        if not match.empty:
            print("DEBUG: Ejercicio encontrado en la base de datos.") # Mensaje de depuraci√≥n
            context = match.iloc[0]['Contenido']
            libro = match.iloc[0]['Libro']
            
            prompt_ejercicio = f"""
            **ROL Y OBJETIVO:** Eres un profesor asistente cuya √∫nica tarea es explicar la soluci√≥n del ejercicio "{exercise_id}" del libro "{libro}".

            **REGLAS CR√çTICAS:**
            1.  Tu conocimiento se limita ESTRICTA Y √öNICAMENTE al siguiente "Contexto de la Soluci√≥n".
            2.  NO PUEDES usar conocimiento externo ni inventar informaci√≥n.
            3.  Tu explicaci√≥n debe ser did√°ctica. Formatea con Markdown y usa LaTeX para las ecuaciones (ej: $ ... $ o $$ ... $$).

            **Contexto de la Soluci√≥n (Tu √∫nica fuente de verdad):**
            ---
            {context}
            ---

            **Explicaci√≥n:**
            """
            response = model_generation.generate_content(prompt_ejercicio)
            return response.text
        else:
            print("DEBUG: Ejercicio NO encontrado en la base de datos.") # Mensaje de depuraci√≥n
            return f"He revisado mis apuntes y no tengo la soluci√≥n para el ejercicio '{exercise_id}'. Para problemas espec√≠ficos, solo puedo usar la informaci√≥n registrada en mi base de datos."

    # --- CAMINO B: PREGUNTA TE√ìRICA O GENERAL (PERSONALIDAD FLEXIBLE) ---
    else:
        print(f"DEBUG: No se detect√≥ ID de ejercicio. Entrando en CAMINO FLEXIBLE para la pregunta: '{query}'") # Mensaje de depuraci√≥n
        
        # En este camino, no necesitamos buscar en nuestra base de datos, ya que el modelo usar√° su conocimiento general.
        # Simplemente le pasamos la pregunta directamente con un prompt que le da libertad.
        
        prompt_teoria = f"""
        **ROL Y OBJETIVO:** Eres un profesor de Ingenier√≠a Civil experto, amigable y apasionado. Tu objetivo es responder a la pregunta de un estudiante de la forma m√°s completa y clara posible.

        **REGLAS CR√çTICAS:**
        1.  **Usa tu conocimiento general:** Tienes total libertad para usar todo tu conocimiento como modelo de IA avanzado para responder a la pregunta.
        2.  **S√© un gran profesor:** Explica los conceptos de forma intuitiva, da ejemplos si es necesario y estructura tu respuesta para que sea f√°cil de seguir.
        3.  **Formato Impecable:** Utiliza Markdown (negritas, listas, etc.) y formatea cualquier ecuaci√≥n, f√≥rmula o variable matem√°tica con LaTeX (ej: $ ... $ o $$ ... $$).

        **Pregunta del Estudiante:**
        ---
        {query}
        ---

        **Tu respuesta como profesor experto:**
        """
        response = model_generation.generate_content(prompt_teoria)
        return response.text

# -------------------
# INTERFAZ DE USUARIO PRINCIPAL (VERSI√ìN ROBUSTA)
# -------------------

# Paso 1: Intentar cargar la base de conocimiento.
# Usamos un bloque try-except para manejar cualquier posible error durante la carga.
try:
    # Esta l√≠nea llama a la funci√≥n que tiene los datos y los procesa.
    # El resultado se guarda en cach√© para no repetirlo.
    df_knowledge = get_knowledge_base() 
    
    # Comprobaci√≥n expl√≠cita para asegurarnos de que el DataFrame no est√° vac√≠o o es inv√°lido.
    if df_knowledge is None or df_knowledge.empty:
        st.error("‚ùå La base de conocimiento no se pudo cargar o est√° vac√≠a. El chatbot no puede funcionar.")
        # Detenemos la ejecuci√≥n aqu√≠ si no hay datos.
        st.stop() 

except Exception as e:
    st.error(f"üö® Ocurri√≥ un error cr√≠tico al inicializar la base de conocimiento: {e}")
    st.warning("Revisa la funci√≥n 'get_knowledge_base' en el c√≥digo fuente.")
    # Detenemos la ejecuci√≥n si hay un error.
    st.stop()


# Si el c√≥digo llega hasta aqu√≠, significa que df_knowledge se carg√≥ correctamente.
# Ahora podemos dibujar la interfaz de chat con seguridad.

st.success("‚úÖ Base de conocimiento cargada. ¬°El profesor est√° listo!")

# Inicializa el historial del chat si no existe.
if 'messages' not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "¬°Hola! Soy tu profesor virtual. ¬øEn qu√© tema o ejercicio necesitas ayuda hoy?"}]

# Muestra todos los mensajes del historial en cada recarga de la p√°gina.
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- LA BARRA DE CHAT ---
# Esta l√≠nea dibuja la barra de entrada de texto en la parte inferior.
# El `if` se ejecuta solo cuando el usuario escribe algo y presiona Enter.
if prompt := st.chat_input("Escribe tu pregunta aqu√≠..."):
    # 1. A√±ade el mensaje del usuario al historial y lo muestra en la pantalla.
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. Genera y muestra la respuesta del asistente.
    with st.chat_message("assistant"):
        # Muestra un indicador de "pensando" mientras se genera la respuesta.
        with st.spinner("Consultando mis apuntes y formulando una respuesta..."):
            response = generate_response(prompt, df_knowledge)
            st.markdown(response)
    
    # 3. A√±ade la respuesta del asistente al historial para que persista.
    st.session_state.messages.append({"role": "assistant", "content": response})
