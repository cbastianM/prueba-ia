# -------------------
# LIBRER√çAS
# -------------------
import streamlit as st
import google.generativeai as genai
import pandas as pd
import numpy as np
import re
import requests
from PIL import Image
import io
# -------------------
# CONFIGURACI√ìN DE LA P√ÅGINA Y API
# -------------------
st.set_page_config(
    page_title="Tu Profesor de Ing. Civil",
    page_icon="üéì",
    layout="centered" # Un layout m√°s enfocado para chat
)

st.title("üéì Tu Profesor Virtual de Ingenier√≠a Civil")
st.markdown("Hazme una pregunta sobre un tema o pide la explicaci√≥n de un ejercicio de la base de conocimiento.")

# Cargar la API Key desde los secrets de Streamlit
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=GEMINI_API_KEY)
except (FileNotFoundError, KeyError):
    st.error("‚ö†Ô∏è No se encontr√≥ la GEMINI_API_KEY. Por favor, config√∫rala en los secrets de Streamlit Cloud.")
    st.stop()

# -------------------
# BASE DE CONOCIMIENTO (DATOS DENTRO DEL C√ìDIGO)
# -------------------

# Reemplaza tu antigua funci√≥n get_knowledge_base() por esta:

@st.cache_resource
def get_knowledge_base():
    """
    Lee un archivo CSV desde una URL p√∫blica (GitHub), lo convierte a DataFrame
    y genera los embeddings. Retorna el DataFrame procesado.
    """
    # --- CONSTRUYE LA URL DEL ARCHIVO CSV EN GITHUB ---
    # Reemplaza 'tu_usuario_github', 'tu_repositorio' y 'main' si es necesario.
    github_user = "cbastianM"
    github_repo = "prueba-ia"
    branch_name = "main" # O 'master', dependiendo de tu repositorio
    file_path = "Conocimiento_Ing_Civil.csv"

    csv_url = f"https://raw.githubusercontent.com/cbastianM/prueba-ia/refs/heads/main/Conocimiento_Ing_Civil.csv"

    try:
        # Lee el archivo CSV directamente desde la URL
        df = pd.read_csv(csv_url)
        
        # Limpieza de datos: reemplaza valores nulos (NaN) por strings vac√≠os
        df.fillna('', inplace=True)
        
        # Generaci√≥n de embeddings
        model_embedding = 'models/embedding-001'
        
        # Usamos una barra de progreso para dar feedback al usuario durante la carga inicial
        progress_bar = st.progress(0, text="Analizando base de conocimiento...")
        
        def get_embedding(text, index):
            # Actualiza la barra de progreso
            progress_bar.progress((index + 1) / len(df), text=f"Procesando entrada {index+1}/{len(df)}...")
            if not isinstance(text, str) or not text.strip():
                return [0.0] * 768
            return genai.embed_content(
                model=model_embedding, content=text, task_type="RETRIEVAL_DOCUMENT")["embedding"]

        # Aplica la funci√≥n de embedding
        df['Embedding'] = [get_embedding(row['Contenido'], i) for i, row in df.iterrows()]
        
        # Limpia la barra de progreso una vez terminado
        progress_bar.empty()
        
        return df

    except Exception as e:
        st.error(f"Error al cargar o procesar el archivo CSV desde GitHub: {e}")
        st.warning("Verifica que la URL sea correcta y que el archivo CSV est√© en el repositorio.")
        return None


# -------------------
# L√ìGICA DEL CHATBOT MULTIMODAL
# -------------------
def extract_exercise_id(query):
    """Extrae un ID de ejercicio como 'BEER 2.73' de la pregunta."""
    books = ["beer", "hibbeler", "singer", "gere", "chopra", "irving"]
    pattern = re.compile(
        r'\b(' + '|'.join(books) + r')[\s\w\.]*(\d+[\.\-]\d+)\b', re.IGNORECASE)
    match = pattern.search(query)
    if match:
        book_name, exercise_num = match.group(1).upper(), match.group(2).replace('-', '.')
        return f"{book_name} {exercise_num}"
    return None

# -------------------
# INTERFAZ DE USUARIO PRINCIPAL (Modificada para mostrar m√∫ltiples im√°genes)
# -------------------
df_knowledge = get_knowledge_base()

if df_knowledge is not None:
    st.success("‚úÖ Base de conocimiento cargada. ¬°El profesor est√° listo!")
    
    if 'messages' not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "¬°Hola! ¬øEn qu√© puedo ayudarte hoy?"}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if "images" in message:  # Busca la clave "images" (plural)
                for i, img in enumerate(message["images"]): # Itera sobre la lista de im√°genes
                    st.image(img, caption=f"Imagen {i+1}", use_column_width=True) # A√±adido use_column_width
            st.markdown(message["content"])

    if prompt := st.chat_input("Escribe tu pregunta aqu√≠..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Analizando texto e im√°genes..."):
                response_text, response_images = generate_response(prompt, df_knowledge) # Recibe la lista de im√°genes
                
                # Prepara el mensaje para el historial (con la lista de im√°genes)
                assistant_message = {"role": "assistant", "content": response_text, "images": response_images}
                
                # Muestra cada imagen con su caption
                if response_images:
                    for i, img in enumerate(response_images):
                        st.image(img, caption=f"Imagen {i+1} de referencia", use_column_width=True) # A√±adido use_column_width
                
                st.markdown(response_text)
                st.session_state.messages.append(assistant_message)
else:
    st.error("La base de conocimiento no pudo ser cargada. Revisa la URL del CSV y los logs.")
